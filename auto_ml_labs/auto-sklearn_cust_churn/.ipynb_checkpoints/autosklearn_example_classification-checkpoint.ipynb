{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction using Auto-SKLearn\n",
    "_**Using Auto-SKLearn to Predict Mobile Customer Departure**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "Kernel `conda_mxnet_latest_p37` works well with this notebook.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Classification](#Data)\n",
    "1. [Data Loading](#Dataloading)\n",
    "1. [Auto-SKLearn Results](#Results)\n",
    "1. [Host](#Host)\n",
    "1. [Cleanup](#Cleanup)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction<a name=\"Introduction\"></a>\n",
    "\n",
    "Auto-Sklearn is an open-source library for performing AutoML in Python. It makes use of the popular Scikit-Learn machine learning library for data transforms and machine learning algorithms and uses a Bayesian Optimization search procedure to efficiently discover a top-performing model pipeline for a given dataset. \n",
    "\n",
    "This notebook has been developed to show AutoML development using Auto-SKLearn as a comparison to SageMaker AutoPilot. Currently SageMaker AutoPilot is not available in GovCloud so we are going to use Auto-SKLearn to be able to do AutoML on GovCloud.\n",
    "\n",
    "---\n",
    "## Setup<a name=\"Setup\"></a>\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's start by installing auto-sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install auto-sklearn==0.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification<a name=\"Classification\"></a>\n",
    "\n",
    "The following example shows how to fit a simple classification model with\n",
    "*auto-sklearn*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import autosklearn.classification\n",
    "\n",
    "# You can modify the following to use a bucket of your choosing\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/autopilot-cust-churn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading<a name=\"Dataloading\"></a>\n",
    "\n",
    "We already have a prepared dataset from our AutoPilot lab we just completed. Now we will load our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"s3://{bucket}/{prefix}/train/train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test to see if the dataset files are in S3. If not then the AutoPilot lab was not run ahead of time and so the file are re-created and stored in S3. This situation would occur if running the labs in the GovCloud region since AutoPilot is not available yet. This next cell will allow you to run the auto-sklearn labs without having to run the AutoPilot lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "src_prefix = \"sagemaker/autopilot-cust-churn\"\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "print(f\"Testing if file exists at s3://{bucket}/{src_prefix}/train/train_data.csv\")\n",
    "\n",
    "try:\n",
    "    s3.Object(f'{bucket}', f'{src_prefix}/train/train_data.csv').load()\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == \"404\":\n",
    "        # The object does not exist.\n",
    "        churn = pd.read_csv(\"../dataset/churn.txt\", index_col=False)\n",
    "        churn.drop(columns=['Area Code', 'Phone'],inplace=True)\n",
    "        \n",
    "        train_data = churn.sample(frac=0.8, random_state=200)\n",
    "        test_data = churn.drop(train_data.index)\n",
    "        test_data_no_target = test_data.drop(columns=[\"Churn?\"])\n",
    "        \n",
    "        train_file = \"train_data.csv\"\n",
    "        train_data.to_csv(train_file, index=False, header=True)\n",
    "        train_data_s3_path = session.upload_data(path=train_file, key_prefix=src_prefix + \"/train\")\n",
    "        print(\"Train data uploaded to: \" + train_data_s3_path)\n",
    "\n",
    "        test_file = \"test_data.csv\"\n",
    "        test_data_no_target.to_csv(test_file, index=False, header=False)\n",
    "        test_data_s3_path = session.upload_data(path=test_file, key_prefix=src_prefix + \"/test\")\n",
    "        print(\"Test data uploaded to: \" + test_data_s3_path)\n",
    "\n",
    "        test_file_wt = \"test_data_w_target.csv\"\n",
    "        test_data.to_csv(test_file_wt, index=False, header=True)\n",
    "        test_data_s3_path = session.upload_data(path=test_file_wt, key_prefix=src_prefix + \"/test\")\n",
    "        print(\"Test data with target uploaded to: \" + test_data_s3_path)\n",
    "    else:\n",
    "        # Something else has gone wrong.\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now load the training dataset from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "response = s3_client.get_object(Bucket=bucket, Key=f\"{prefix}/train/train_data.csv\")\n",
    "\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    data = pd.read_csv(response.get(\"Body\"),index_col=False)\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the loaded dataframe, we now create our training dataset (train_X) and our target dataset (train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data[[c for c in data.columns if c != 'Churn?']]\n",
    "train_Y = data[['Churn?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training dataset, the columns \"State\", \"Int'l Plan\" and \"VMail Plan\" are strings. So we set their type as 'category' and auto-sklearn will perform categorical encoding on them. NOTE: Without this step, auto-sklearn will fail. This was not needed when we used AutoPilot since it is able to analyze and apply the correct encoding as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[\"State\"] = train_X[\"State\"].astype('category')\n",
    "train_X[\"Int'l Plan\"] = train_X[\"Int'l Plan\"].astype('category')\n",
    "train_X[\"VMail Plan\"] = train_X[\"VMail Plan\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and fit a classifier\n",
    "\n",
    "Now we have our dataset ready, we set up an auto-sklearn classifier since we are doing Binary Classification. \n",
    "The parameters are as follows\n",
    "\n",
    "**time_left_for_this_task** = Time limit in seconds for the search of appropriate models. By increasing this value, auto-sklearn has a higher chance of finding better models\n",
    "\n",
    "**per_run_time_limit** = Time limit for a single call to the machine learning model\n",
    "\n",
    "**tmp_folder** = folder to store configuration output and log files\n",
    "\n",
    "We have set these values lower to ensure the jobs complete with the workshop timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=120,\n",
    "    per_run_time_limit=30,\n",
    "    tmp_folder='/tmp/autosklearn_classification_example_tmp2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the **fit** method with the training and target datasets. (This process takes a few minutes to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "automl.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the models found by auto-sklearn\n",
    "\n",
    "Now the that auto-sklearn fit process has finished, lets evaluate what it found. We can list the ensemble models its created with their relevant statistics by calling the **leaderboard** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(automl.leaderboard())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the final ensemble constructed by auto-sklearn\n",
    "\n",
    "Returns a representation of the final ensemble found by auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(automl.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Score of the final ensemble\n",
    "\n",
    "Now let us load the test dataset that we created and stored in S3. We are going to use the test dataset with the target column since we will use this column data later to evaluate the accuracy of thee predictions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.get_object(Bucket=bucket, Key=f\"{prefix}/test/test_data_w_target.csv\")\n",
    "\n",
    "status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "if status == 200:\n",
    "    print(f\"Successful S3 get_object response. Status - {status}\")\n",
    "    test_data = pd.read_csv(response.get(\"Body\"),index_col=False)\n",
    "else:\n",
    "    print(f\"Unsuccessful S3 get_object response. Status - {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create the testing dataset in X_test) and strip off the target column (Churn? column into y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data[[c for c in data.columns if c != 'Churn?']]\n",
    "y_test = test_data[['Churn?']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at the structure of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do predictions using the **X_test** dataset and then compare the results against **y_test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = automl.predict(X_test)\n",
    "print(\"Accuracy score:\", sklearn.metrics.accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save the best performing ensemble model\n",
    "\n",
    "The automl variable contains the trained model. We are going to save the model and model artifects of the best performing ensemble model in pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\".\", 'auto-sklearn.pkl'), 'wb') as out:\n",
    "    pickle.dump(automl, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Lab 2\n",
    "\n",
    "So what we have done in this lab is to:\n",
    "\n",
    "1) Take your dataset and do some data engineering (we removed some columns) and provided hints to auto-sklearn (indicating categorical columns)\n",
    "\n",
    "2) auto-sklearn analyzed the dataset and evaluated different permutations and combinations of algorithms to create the best performing model\n",
    "\n",
    "3) Once the process was finished, we could use a test dataset to do live predictions to assess the accuracy of our model.\n",
    "\n",
    "Since this is a workshop, all of the work was done in the notebook using small datasets and constrained execution time limits. This method works great for development, however does not work for productionizing a process. In the next lab we will show how to use the power of SageMaker to make the auto-sklearn training and inference into fully managed services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
